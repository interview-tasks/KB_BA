# Business Analytics Techniques Reference Table

| # | Technique | Purpose & Core Outputs | Step-by-Step Workflow | Minimal Data Schema | Example SQL/Methods | Visualization | Common Pitfalls | Decision Triggers |
|---|-----------|------------------------|----------------------|-------------------|-------------------|---------------|-----------------|-------------------|
| 1 | **RFM Analysis (Recency, Frequency, Monetary)** | Segment customers into behavioral cohorts for lifecycle marketing<br>**Outputs:** R, F, M scores (1-5), RFM codes, segment lists (VIP, Loyal, At-risk) | 1. Choose time window (e.g., 12 months)<br>2. Calculate recency_days, frequency, monetary per customer<br>3. Bin each metric into quintiles<br>4. Assign RFM codes and map to segments<br>5. Validate segments by revenue uplift | `orders(order_id, customer_id, order_date, order_value)`<br>`customers(customer_id, signup_date)` | ```sql<br>WITH rfm AS (<br> SELECT customer_id,<br>  EXTRACT(day FROM (CURRENT_DATE - MAX(order_date)))::int AS recency_days,<br>  COUNT(*) AS frequency,<br>  SUM(order_value) AS monetary<br> FROM orders WHERE order_date >= (CURRENT_DATE - INTERVAL '12 months')<br> GROUP BY customer_id<br>)<br>SELECT *, ntile(5) OVER (ORDER BY recency_days ASC) AS r_score,<br>          ntile(5) OVER (ORDER BY frequency DESC) AS f_score,<br>          ntile(5) OVER (ORDER BY monetary DESC) AS m_score<br>FROM rfm;<br>``` | 3D scatter or treemap of RFM segments, cohort revenue per segment | Using arbitrary bins instead of quantiles; inverting recency logic | Target top-10% RFM customers with referral invites; target at-risk segments with winback coupons |
| 2 | **Cohort Analysis** | Measure retention, LTV, and behavior per acquisition cohort<br>**Outputs:** Retention matrices, cohort LTV curves, survival functions | 1. Define cohort (e.g., acquisition month)<br>2. Compute cohort size and activity in subsequent periods<br>3. Build retention matrix (% retained)<br>4. Calculate cumulative revenue per cohort member<br>5. Compare cohorts across channels and time | `customers(customer_id, acquired_date, acquisition_channel)`<br>`orders(order_id, customer_id, order_date, order_value)` | ```sql<br>SELECT<br> date_trunc('month', acquired_date) AS cohort_month,<br> date_trunc('month', order_date) AS order_month,<br> COUNT(DISTINCT customer_id) AS active_customers<br>FROM customers c<br>JOIN orders o ON c.customer_id = o.customer_id<br>GROUP BY cohort_month, order_month;<br>``` | Cohort heatmap (months on x, cohorts on y) with retention % shades; cohort LTV curves | Mixing cohorts from different channels; small cohorts lead to noisy percentages | If new cohorts have materially worse retention, pause scaling until onboarding fixed |
| 3 | **A/B Testing** | Establish causal effect of product/marketing changes<br>**Outputs:** Lift, statistical significance (p-value), confidence intervals, estimated revenue impact | 1. Define primary metric and minimum detectable effect<br>2. Compute required sample size<br>3. Randomize users to control/variant<br>4. Run test for pre-specified duration<br>5. Compute statistical tests and report results<br>6. Evaluate practical significance | `exposures(user_id, variant, exposure_date)`<br>`outcomes(user_id, conversion_flag, revenue)` | ```sql<br>SELECT variant,<br>       COUNT(*) AS visitors,<br>       SUM(conversion_flag) AS conversions,<br>       SUM(conversion_flag)::float / COUNT(*) AS conv_rate<br>FROM exposures e<br>LEFT JOIN outcomes o USING (user_id)<br>GROUP BY variant;<br>``` | Bar chart with error bars, cumulative lift plot | Under-powering test, peeking without correction, post-hoc segmentation (p-hacking) | Adopt variant when lift is statistically significant and ROI-positive |
| 4 | **Survival/Churn Modeling** | Estimate time to churn and compare survival across groups<br>**Outputs:** Survival curve, median lifetime, hazard ratios | 1. Build customer dataset with start_date, end_date, event flag<br>2. Fit Kaplan-Meier to estimate survival function<br>3. Use Cox proportional hazards for covariate effects<br>4. Test assumptions and validate model | `customers(customer_id, start_date)`<br>`events(customer_id, event_date, event_type)` | Construct dataset in SQL then export to R/Python for Kaplan-Meier or Cox regression | Survival curve with strata (channels) overlays | Non-proportional hazards violating Cox assumptions | Identify segments with fast decay for prioritized retention |
| 5 | **Incrementality/Holdout Testing** | Measure true incremental effect of marketing spend<br>**Outputs:** Incremental conversions/revenue, incremental ROAS | 1. Create randomized holdout groups (10-20%)<br>2. Run campaign on exposed group<br>3. Measure conversion differences<br>4. Compute incremental revenue and attribute to campaign | `users(user_id, exposure_flag, outcome_metric)` | Same as A/B testing SQL - compare averages by exposure_flag | Incremental revenue bar showing baseline vs exposed | Spillover effects, non-random exposure assignment | If incremental ROAS < benchmark, reallocate budget |
| 6 | **Attribution Modeling** | Allocate credit to multiple touchpoints in conversion paths<br>**Outputs:** Channel contribution percentages, removal effect | 1. Parse ordered touchpoint paths per user<br>2. Build Markov transition matrix<br>3. Compute removal effect by comparing conversion probability with/without each channel<br>4. Validate with hold-out or incrementality tests | `touchpoints(user_id, event_time, channel, is_conversion)` | Build path strings per user in SQL, then export for Markov/Shapley calculation | Sankey diagram of conversion paths; channel contribution table | Large path space; data sparsity for rare channels | Re-weight channel budget based on marginal contribution |
| 7 | **Propensity Modeling** | Predict probability of outcome (churn, conversion) to prioritize actions<br>**Outputs:** Probability scores, lift charts, calibration plots | 1. Define label (churn event window)<br>2. Feature engineering (RFM, engagement, billing)<br>3. Split data; train model (logistic, XGBoost)<br>4. Validate with ROC/AUC and calibration<br>5. Deploy scores to targeting engine | `customers`, `events`, `billing` tables | Aggregate features in SQL then export to modeling environment | ROC curve, precision-recall, decile lift | Label leakage, temporal cross-validation missteps | Target top decile with retention offers if ROI positive |
| 8 | **Uplift Modeling** | Find who is most likely to respond positively to treatment<br>**Outputs:** Individual treatment effect scores | 1. Run randomized treatment<br>2. Train uplift models (two-model approach, causal forests)<br>3. Rank customers by estimated uplift<br>4. Validate with holdout experiments | `experiments(user_id, treated_flag, outcome)` | Causal forests, uplift trees, S-learner/T-learner | Treatment effect distribution plots, uplift by decile | Requires randomized treatment or strong assumptions | Send promotions only to users with positive predicted uplift |
| 9 | **Time-Series Forecasting** | Forecast revenue/demand and decompose into trend/seasonality/noise<br>**Outputs:** Forecast with confidence intervals, seasonality profile | 1. Aggregate series (daily/weekly/monthly)<br>2. Decompose (STL/seasonal) and test stationarity<br>3. Fit model (ETS/ARIMA/Prophet)<br>4. Backtest and compute MAPE/RMSE<br>5. Generate forecasts with confidence bands | `timeseries(date, metric_value)` | Aggregate in SQL then model in Python/R (Prophet, ARIMA, etc.) | Forecast plot with confidence bands; decomposed trend and seasonality panels | Overfitting; ignoring external regressors (holidays, promotions) | Adjust inventory/hiring based on forecasted peaks |
| 10 | **Product Analytics** | Understand user behavior related to product features and value realization<br>**Outputs:** Adoption rate, feature stickiness, retention lift per feature | 1. Define eligibility for feature<br>2. Compute adoption = users who used feature รท eligible<br>3. Use matching or A/B to estimate causal effect on retention<br>4. Measure feature stickiness over time | `events(user_id, event_name, event_time)`<br>`user_properties` | Count distinct users by event grouping | Adoption curves, retention curves segmented by feature use | Confounding (power users both use feature and retain) | Promote features that show causal retention uplift |
| 11 | **Anomaly Detection** | Early detection of metric deviations (traffic dips, tracking errors)<br>**Outputs:** Alerts, anomaly logs, root-cause candidates | 1. Define baseline model (rolling mean, seasonal decomposition)<br>2. Set thresholds (z-score, median absolute deviation)<br>3. Trigger alert and auto-collect context<br>4. Investigate and document findings | `metrics(date_time, metric_name, value, segment)` | ```sql<br>WITH stats AS (<br> SELECT AVG(value) AS mu, STDDEV(value) AS sigma<br> FROM metrics WHERE metric_name = 'daily_visitors'<br> AND date_time >= current_date - interval '30 days'<br>)<br>SELECT m.*, (m.value - s.mu)/s.sigma AS z<br>FROM metrics m CROSS JOIN stats s<br>WHERE m.metric_name = 'daily_visitors'<br>AND ABS((m.value - s.mu)/s.sigma) > 3;<br>``` | Time-series with flagged anomalies and drill-down panels | Too sensitive thresholds creating alert fatigue | Block deployments or investigate tracking if conversion drops >X SD |
| 12 | **Pricing & Elasticity Analysis** | Estimate price elasticity to set optimal prices and discounts<br>**Outputs:** Elasticity estimate, optimal price point | 1. Run price experiments (A/B or geographically controlled)<br>2. Fit log-log model: ln(q) = a + b ln(p); elasticity = b<br>3. Simulate revenue at candidate prices<br>4. Account for competitive responses | `sales(product_id, price, quantity, region, date)` | Log-log regression in R/Python: `lm(log(quantity) ~ log(price) + controls)` | Demand curve (price vs quantity), revenue by price | Confounding promotions and seasonality; not holding other factors constant | Change price if elasticity indicates revenue increase potential |
| 13 | **SKU/Product Portfolio Analysis** | Identify best-selling and margin-contributing SKUs and measure cannibalization<br>**Outputs:** Pareto curves, cannibalization matrices | 1. Compute revenue and margin by SKU<br>2. Rank and compute cumulative percent<br>3. For cannibalization, compare SKU mix pre/post launch<br>4. Run controlled rollouts | `orders(order_id, sku_id, sku_price, quantity, order_date)` | ```sql<br>SELECT sku_id,<br>       SUM(sku_price * quantity) AS revenue,<br>       COUNT(DISTINCT order_id) AS orders,<br>       SUM(SUM(sku_price * quantity)) OVER (ORDER BY SUM(sku_price * quantity) DESC<br>           ROWS UNBOUNDED PRECEDING) / SUM(SUM(sku_price * quantity)) OVER () AS cumulative_pct<br>FROM orders GROUP BY sku_id ORDER BY revenue DESC;<br>``` | Pareto chart (bar + cumulative line), heatmaps for cannibalization | Seasonality and promotions can distort comparisons | Delist low-contribution SKUs; expand top SKUs |
| 14 | **Financial Modeling** | Combine metrics into P&L forecasts and scenario stress-tests<br>**Outputs:** Unit-economics tables, scenario P&L, break-even analysis | 1. Build per-customer revenue and cost model<br>2. Project acquisition and retention under scenarios<br>3. Compute cashflow and runway under scenarios<br>4. Stress-test key assumptions | Aggregated metrics and assumptions tables | Build in Excel/Python: cohort LTV models, acquisition funnels, cost structure | Waterfall P&L, scenario comparison charts | Garbage-in garbage-out - be explicit with assumptions | Fundraise timing, hiring, expansion decisions |

## Quick Reference: Which Technique for Common Business Questions

| Business Question | Recommended Technique(s) |
|-------------------|-------------------------|
| **"Which channel is most profitable?"** | Cohort LTV by channel + Attribution + Incrementality |
| **"Are our experiments increasing revenue?"** | A/B testing with revenue as primary metric + Uplift analysis |
| **"What's the expected revenue from this acquisition run?"** | Cohort LTV forecast + CAC payback + Churn assumptions |
| **"Which customers should CSM call today?"** | Propensity-to-churn model + Customer health score |
| **"Is our product 'sticky'?"** | DAU/MAU + Retention cohorts + Time-to-first-value metrics |
| **"Which feature drives retention?"** | Product adoption analysis + Uplift modeling or randomized experiment |
| **"Should we raise prices?"** | Pricing & Elasticity Analysis + A/B testing |
| **"What's our revenue forecast for next quarter?"** | Time-series forecasting + Financial modeling |
| **"Which products should we discontinue?"** | SKU/Product Portfolio Analysis (Pareto) |
| **"Is our marketing spend working?"** | Incrementality/Holdout testing + Attribution modeling |

## Statistical Methods & Tools Summary

| Technique | Primary Statistical Methods | Recommended Tools |
|-----------|----------------------------|-------------------|
| **RFM Analysis** | Quantile binning, clustering | SQL + Excel/Tableau |
| **Cohort Analysis** | Time-series aggregation, survival analysis | SQL + Python/R |
| **A/B Testing** | Frequentist z/t-tests, Bayesian alternatives | SQL + Python (scipy, statsmodels) |
| **Survival Analysis** | Kaplan-Meier estimator, Cox PH regression | R (survival package), Python (lifelines) |
| **Attribution** | Markov chains, Shapley values | Python (attribution libraries) |
| **Propensity Modeling** | Logistic regression, XGBoost, Random Forest | Python (scikit-learn, xgboost) |
| **Uplift Modeling** | Causal forests, two-model approach | Python (causalml), R (uplift) |
| **Time-Series** | ARIMA, ETS, Prophet, STL decomposition | Python (Prophet, statsmodels), R (forecast) |
| **Anomaly Detection** | Z-score, Isolation Forest, seasonal decomposition | Python (scikit-learn), SQL |
